import cv2
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier

#1. створюємо функцію для генерації фігур
def generate_image(color, shape):
    img = np.zeros((200, 200, 3), np.uint8)
    if shape == 'circle':
        cv2.circle(img, (100, 100), 50, color, -1)
    elif shape == 'square':
        cv2.rectangle(img, (50, 50), (150, 150), color, -1)
    elif shape == 'triangle':
        points = np.array([[100, 40], [40, 160], [160, 160]])
        cv2.drawContours(img, [points], 0, color, -1)

    return img

#2. формуємо набір даних
X = [] #список ознак
y = [] #cписок міток

colors = {
    'red' : (255, 0, 0),
    'green' : (0, 255, 0),
    'blue' : (0, 0, 255),
    'yellow' : (255, 255, 0)
}

shapes = ['circle', 'square', 'triangle']
for color in colors.items():
    for shape in shapes:
        for _ in range(10):
            img = generate_image(color, shape)
            mean_color = cv2.mean(img)[:3] #B, G, R, alpha
            features = [mean_color[0], mean_color[1], mean_color[2]]  # ознаки тут — середній колір по каналах B, G, R:
            X.append(features) #усі ознаки (features), тобто числові дані, за якими навчається модель
            # в нашому випадку це одна з трьох фігур + значення кольору
            y.append(f"{color}_{shape}") #усі мітки (labels), тобто правильні відповіді, які модель повинна передбачати

#3 розділяємо дані 70% даних — для навчання, 30% — для перевірки
#У машинному навчанні ми ніколи не тренуємо модель на всіх даних одразу.
#Бо якщо вона “вивчить” усе напам’ять — ми не знатимемо, чи справді
#вона вміє узагальнювати, чи просто запам’ятала приклади.
#Тому ми ділимо весь набір даних (dataset) на дві частини:
#X_train, y_train — дані для навчання моделі, X_test, y_test — дані для перевірки (тестування)
#X_train -ознаки (features) для навчання, модель бачить їх і вчиться
#y_train - правильні відповіді (labels) для навчання, щоб модель знала, що є правильним
#X_test - ознаки для перевірки, нові дані, яких модель "не бачила"
#y_test	правильні відповіді для перевірки, щоб оцінити, наскільки модель вгадує

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, stratify = y)
#stratify=y — зберігаємо однакову пропорцію класів у train і test (важливо, щоб оцінка була чесною).
#Без stratify можна випадково отримати дисбаланс: наприклад,
#у тесті з'являться майже лише квадрати, і метрика спотвориться.

#4 створюємо та навчаємо модель. Вона вчиться порівнювати об'єкти за схожістю кольорів
#k-Nearest Neighbors (KNN) — це дуже простий класифікатор:
# щоб визначити клас нового прикладу, він дивиться на k найближчих
#навчальних прикладів у просторі ознак і обирає більшість
model = KNeighborsClassifier(n_neighbors=3)#беремо 3 найближчих навчальних приклади
model.fit(X_train, y_train)#запам’ятали тренувальні приклади (і побудували структуру пошуку)

#4 перевірка точності
accuracy = model.score(X_test, y_test)#score для класифікатора — це accuracy частка вірних відповідей на тесті
print("Точність моделі:", round(accuracy * 100, 2), "%")

#5 тестуємо модель на новому зображенні
test_img = generate_image((0, 255, 0), 'circle')
mean_color = cv2.mean(test_img)[:3]
prediction = model.predict([mean_color])
print("Передбачення:", prediction[0])

cv2.imshow("Test image", test_img)
cv2.waitKey(0)
cv2.destroyAllWindows()

#КОНСПЕКТ. Термін / функція - Пояснення
# Датасет (dataset) — набір навчальних прикладів, з яких
# модель вчиться (у нас — зображення фігур різних кольорів).
#
# Зразок / приклад (sample) — одне зображення або один
# набір ознак.
#
# Ознаки (features) — числові характеристики зображення,
# за якими модель робить висновки (у нашому коді — середні
# значення кольорів B, G, R).
#
# Мітка (label) — правильна назва або клас прикладу
# (наприклад: “red_circle”, “blue_square”).
#
# Клас (class) — категорія, до якої належить приклад.
#
# Навчання (training) — процес, коли модель вчиться на
# даних розпізнавати закономірності між ознаками і класами.
#
# Передбачення (prediction) — визначення класу для нового
# невідомого зразка.
#
# K-Nearest Neighbors (KNN) — алгоритм, який порівнює
# новий зразок з кількома найближчими прикладами з
# навчальних даних і обирає клас більшості.
#
# k (кількість сусідів) — параметр KNN; скільки
# найближчих прикладів враховується при передбаченні
#
# (у нас — 3).
#
# Train/Test split — поділ даних на частини:
#
# train — для навчання моделі,
#
# test — для перевірки точності.
#
# Точність (accuracy) — відсоток правильних передбачень
# моделі на тестових даних.
#
# Ознаковий простір (feature space) — координатна
# система, де кожен зразок описується своїми ознаками
#
# (у нашому випадку тривимірний простір кольорів B, G, R).


# cv2.MORPH_OPEN mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, np.ones((5,5), np.uint8))
# “відкриття”
#     Що робить:
#     Видаляє маленькі білі точки (шум) навколо.
#     Залишає лише великі об’єкти.
#     Аналогія:
#     якщо на білому аркуші є кілька дрібних плям,
#     “open” прибере дрібне сміття, залишивши
#     великі форми.

# cv2.MORPH_CLOSE mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, np.ones((5,5), np.uint8))
# “закриття”
#     Що робить:
#     Закриває маленькі чорні дірки всередині
#     білих об’єктів.
#     З’єднує фрагменти, якщо об’єкт
#     розірваний на частини.
#     Аналогія:
#     якщо в білому колі є кілька чорних “дірочок”,
#     “close” їх закриє.
